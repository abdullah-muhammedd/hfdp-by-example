# Composite & Iterator Pattern â€“ ETL Pipeline Example

This project is part of my Head First Design Patterns (HFDP) learning journey, specifically demonstrating ChapterÂ 9â€™s **Composite** and **Iterator** patterns in a real-world data engineering context.

## ğŸ¯ Overview

Rather than the classic menu or GUI examples, this codebase constructs a full **ETL (Extract, Transform, Load)** pipeline that:

- **Extracts** sensor data from a CSV file
- **Cleans** and normalizes values (nulls, outâ€‘ofâ€‘range clipping)
- **Transforms** into a consistent schema
- **Loads** both a humanâ€‘readable summary and a raw JSON dump

Each processing unit is a **Job** implementing a common `Executable` interface, and can be composed into higherâ€‘level `PipelineJob` instances (Composite). The pipeline itself is traversed in depthâ€‘first order via an **Iterator**, so the main runner can invoke each leaf job uniformly.

## ğŸ—ï¸ Project Structure

```
.
â”œâ”€â”€ files/                      # Raw input data
â”‚   â””â”€â”€ sensor_readings.csv
â”œâ”€â”€ output/                     # Pipeline outputs
â”‚   â”œâ”€â”€ full.json
â”‚   â””â”€â”€ summary.txt
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ jobs/                   # Concrete and composite job implementations
â”‚   â”‚   â”œâ”€â”€ clean.job.ts
â”‚   â”‚   â”œâ”€â”€ data.job.ts
â”‚   â”‚   â”œâ”€â”€ extract.job.ts
â”‚   â”‚   â”œâ”€â”€ load.job.ts
â”‚   â”‚   â”œâ”€â”€ pipeline.job.ts
â”‚   â”‚   â””â”€â”€ transform.job.ts
â”‚   â””â”€â”€ types/
â”‚       â””â”€â”€ interface/
â”‚           â””â”€â”€ executable.interface.ts
â”œâ”€â”€ main.ts                     # Entry point: composes & runs the ETL pipeline
â”œâ”€â”€ package.json
â””â”€â”€ tsconfig.json
```

## ğŸ¨ Design Pattern Implementation

### Composite Pattern

- **Component**: `Executable` (leaf or composite)
- **Leaf**: `DataJob` subclasses (`ExtractJob`, `CleanJob`, `TransformJob`, `LoadJob`)
- **Composite**: `PipelineJob` holds a list of `Executable` children
- **Benefit**: Treat individual jobs and grouped pipelines uniformly

### Iterator Pattern

- **Iterable**: `PipelineJob` implements `[Symbol.iterator]()`
- **Traversal**: Depthâ€‘first search yields every `Executable` in order
- **Benefit**: Main runner can simply `for (const job of pipeline)` to execute each leaf

## ğŸš€ Getting Started

### Prerequisites

- Node.jsÂ 16+ (for TypeScript & `fs/promises`)
- Yarn or npm
- bun installed

### Installation

```bash
# Clone the repository
git clone <repo-url>

# Install dependencies
yarn install
```

### Running the Demo

```bash
#run the ETL pipeline
yarn dev
```

> Ensure `./files/sensor_readings.csv` exists before running.

## ğŸ’¡ Usage Examples

```typescript
import { PipelineJob } from './src/jobs/pipeline.job';
import { ExtractJob } from './src/jobs/extract.job';
import { CleanJob } from './src/jobs/clean.job';
import { TransformJob } from './src/jobs/transform.job';
import { LoadJob } from './src/jobs/load.job';

async function run() {
  const extract = new ExtractJob('./files/sensor_readings.csv');
  const clean = new CleanJob([]);
  const transform = new TransformJob([]);
  const load = new LoadJob([], './output/summary.txt', './output/full.json.txt');

  const prepareStage = new PipelineJob('Prepare Stage');
  prepareStage.add(clean);
  prepareStage.add(transform);

  const fullPipeline = new PipelineJob('Full ETL Pipeline');
  fullPipeline.add(extract);
  fullPipeline.add(prepareStage);
  fullPipeline.add(load);

  let data: any = null;
  for (const job of fullPipeline) {
    if (job.setInput) job.setInput(data);
    data = await job.execute();
  }

  console.log('âœ… ETL Pipeline completed');
}

run();
```

## ğŸ” Key Learning Points

1. **Composite Pattern**
   - Uniform API for single jobs vs. pipelines
   - Easy nesting & extension

2. **Iterator Pattern**
   - Depthâ€‘first traversal without exposing internal structure
   - Simplifies execution loop

3. **ETL Pipeline Mechanics**
   - Real data handling: file I/O, parsing, cleaning, formatting, output
   - Directory creation & error handling in Load step

## ğŸ“ Notes

- Leaf jobs throw if `add()` is invoked, preserving Composite integrity.
- Output directories are created dynamically if missing.
- CSV parsing uses `csv-parse` for streaming performance.

## ğŸ¤ Contributing

Feel free to extend this with:

- Additional jobs (e.g., validation, enrichment)
- Parallel or branching pipelines
- Integration with databases or message queues
- More robust error/retry decorators

## ğŸ“š Related HFDP Concepts

- **Composite Pattern** (ChapterÂ 9)
- **Iterator Pattern** (ChapterÂ 9)
- Open/Closed & Single Responsibility Principles

## ğŸ“„ License

MIT
